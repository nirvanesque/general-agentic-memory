# GAM Benchmarks

GAM (General Agentic Memory) æ¡†æ¶çš„è¯„ä¼°åŸºå‡†å¥—ä»¶ã€‚

## ğŸ“‹ æ”¯æŒçš„æ•°æ®é›†

| æ•°æ®é›† | ç±»å‹ | æè¿° | æŒ‡æ ‡ |
|--------|------|------|------|
| **HotpotQA** | å¤šè·³é—®ç­” | éœ€è¦è·¨å¤šä¸ªæ–‡æ¡£æ¨ç†çš„é—®ç­”ä»»åŠ¡ | F1 |
| **NarrativeQA** | å™äº‹é—®ç­” | åŸºäºé•¿ç¯‡æ•…äº‹å’Œæ–‡æ¡£çš„é˜…è¯»ç†è§£ | F1 |
| **LoCoMo** | å¯¹è¯è®°å¿† | é•¿å¯¹è¯å†å²ä¸­çš„è®°å¿†æ£€ç´¢ | F1, BLEU-1 |
| **RULER** | é•¿ä¸Šä¸‹æ–‡ | æµ‹è¯•é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„å¤šç§ä»»åŠ¡ | Accuracy |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. å‡†å¤‡æ•°æ®

```bash
# ä¸‹è½½æ•°æ®é›†åˆ° data/ ç›®å½•
mkdir -p data

# HotpotQA
# ä¸‹è½½å¹¶æ”¾ç½®åˆ° data/hotpotqa.json

# NarrativeQA (ä¼šè‡ªåŠ¨ä» HuggingFace ä¸‹è½½)

# LoCoMo
# ä¸‹è½½å¹¶æ”¾ç½®åˆ° data/locomo.json

# RULER
# ä¸‹è½½å¹¶æ”¾ç½®åˆ° data/ruler.jsonl
```

### 3. è®¾ç½®ç¯å¢ƒå˜é‡

```bash
# å¦‚æœä½¿ç”¨ OpenAI API
export OPENAI_API_KEY="your_api_key_here"

# å¯é€‰ï¼šä½¿ç”¨è‡ªå®šä¹‰ API endpoint
export OPENAI_API_BASE="https://your-api-endpoint.com/v1"
```

### 4. è¿è¡Œè¯„ä¼°

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Shell è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# HotpotQA
bash scripts/eval_hotpotqa.sh

# NarrativeQA
bash scripts/eval_narrativeqa.sh

# LoCoMo
bash scripts/eval_locomo.sh

# RULER
bash scripts/eval_ruler.sh --dataset-name niah_single_1

# è¿è¡Œæ‰€æœ‰è¯„ä¼°
bash scripts/eval_all.sh
```

#### æ–¹å¼äºŒï¼šä½¿ç”¨ Python CLI

```bash
# HotpotQA
python -m eval.run \
    --dataset hotpotqa \
    --data-path data/hotpotqa.json \
    --generator openai \
    --model gpt-4 \
    --retriever dense

# NarrativeQA
python -m eval.run \
    --dataset narrativeqa \
    --data-path narrativeqa \
    --max-samples 100

# LoCoMo
python -m eval.run \
    --dataset locomo \
    --data-path data/locomo.json

# RULER
python -m eval.run \
    --dataset ruler \
    --data-path data/ruler.jsonl \
    --dataset-name niah_single_1
```

## ğŸ“Š é…ç½®é€‰é¡¹

### ç”Ÿæˆå™¨ (Generator)

```bash
# ä½¿ç”¨ OpenAI API
--generator openai --model gpt-4 --api-key YOUR_KEY

# ä½¿ç”¨ VLLM (æœ¬åœ°æ¨¡å‹)
--generator vllm --model meta-llama/Llama-3-8B
```

### æ£€ç´¢å™¨ (Retriever)

```bash
# Dense Retriever (è¯­ä¹‰æ£€ç´¢ï¼Œæ¨è)
--retriever dense --embedding-model BAAI/bge-base-en-v1.5

# BM25 Retriever (å…³é”®è¯æ£€ç´¢)
--retriever bm25

# Index Retriever (ç®€å•ç´¢å¼•)
--retriever index
```

### è¯„ä¼°å‚æ•°

```bash
# é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
--max-samples 50

# è°ƒæ•´æ–‡æœ¬å—å¤§å°
--chunk-size 2000

# è°ƒæ•´æ£€ç´¢æ•°é‡
--top-k 5

# è®¾ç½®è¾“å‡ºç›®å½•
--output-dir outputs/my_experiment

# é™é»˜æ¨¡å¼
--quiet

# ä¸ä¿å­˜é¢„æµ‹ç»“æœ
--no-save
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
eval/
â”œâ”€â”€ __init__.py              # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ __main__.py              # å…è®¸ python -m eval.run
â”œâ”€â”€ run.py                   # CLI å…¥å£
â”œâ”€â”€ README.md                # æœ¬æ–‡æ¡£
â”œâ”€â”€ datasets/                # æ•°æ®é›†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py             # åŸºç±»
â”‚   â”œâ”€â”€ hotpotqa.py         # HotpotQA å®ç°
â”‚   â”œâ”€â”€ narrativeqa.py      # NarrativeQA å®ç°
â”‚   â”œâ”€â”€ locomo.py           # LoCoMo å®ç°
â”‚   â””â”€â”€ ruler.py            # RULER å®ç°
â””â”€â”€ utils/                   # å·¥å…·æ¨¡å—
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ chunking.py         # æ–‡æœ¬åˆ‡åˆ†å·¥å…·
    â””â”€â”€ metrics.py          # è¯„ä¼°æŒ‡æ ‡å·¥å…·

scripts/                     # Shell è„šæœ¬
â”œâ”€â”€ eval_hotpotqa.sh
â”œâ”€â”€ eval_narrativeqa.sh
â”œâ”€â”€ eval_locomo.sh
â”œâ”€â”€ eval_ruler.sh
â””â”€â”€ eval_all.sh
```

## ğŸ”§ è‡ªå®šä¹‰è¯„ä¼°

### æ–¹æ³•ä¸€ï¼šä¿®æ”¹ Shell è„šæœ¬å‚æ•°

ç¼–è¾‘ `scripts/eval_*.sh` æ–‡ä»¶ä¸­çš„é»˜è®¤å‚æ•°ï¼š

```bash
# ä¿®æ”¹é»˜è®¤æ¨¡å‹
MODEL="gpt-3.5-turbo"

# ä¿®æ”¹é»˜è®¤æ£€ç´¢å™¨
RETRIEVER="bm25"

# ä¿®æ”¹é»˜è®¤è¾“å‡ºç›®å½•
OUTPUT_DIR="outputs/my_experiment"
```

### æ–¹æ³•äºŒï¼šåˆ›å»ºè‡ªå®šä¹‰ Benchmark

```python
from eval.datasets.base import BaseBenchmark, BenchmarkConfig

class MyBenchmark(BaseBenchmark):
    def load_data(self):
        # å®ç°æ•°æ®åŠ è½½é€»è¾‘
        pass
    
    def prepare_chunks(self, sample):
        # å®ç°æ–‡æœ¬åˆ‡åˆ†é€»è¾‘
        pass
    
    def extract_question(self, sample):
        # æå–é—®é¢˜
        pass
    
    def extract_ground_truth(self, sample):
        # æå–æ ‡å‡†ç­”æ¡ˆ
        pass
    
    def compute_metrics(self, predictions, ground_truths):
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        pass

# ä½¿ç”¨
config = BenchmarkConfig(data_path="my_data.json")
benchmark = MyBenchmark(config)
results = benchmark.run()
```

## ğŸ“ˆ ç»“æœè¾“å‡º

è¯„ä¼°å®Œæˆåï¼Œç»“æœä¼šä¿å­˜åœ¨ `outputs/` ç›®å½•ï¼š

```
outputs/
â”œâ”€â”€ hotpotqa/
â”‚   â””â”€â”€ HotpotQABenchmark_20240116_143022.json
â”œâ”€â”€ narrativeqa/
â”‚   â””â”€â”€ NarrativeQABenchmark_20240116_150533.json
â””â”€â”€ ...
```

ç»“æœæ–‡ä»¶åŒ…å«ï¼š
- é…ç½®ä¿¡æ¯
- è¯„ä¼°æŒ‡æ ‡
- æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹å’Œæ ‡å‡†ç­”æ¡ˆ

ç¤ºä¾‹ç»“æœï¼š

```json
{
  "config": {
    "data_path": "data/hotpotqa.json",
    "generator_type": "openai",
    "model_name": "gpt-4",
    "num_samples": 100
  },
  "metrics": {
    "em": 0.75,
    "f1": 0.83
  },
  "predictions": [
    {
      "prediction": "The answer is...",
      "ground_truth": ["correct answer"]
    }
  ]
}
```

## ğŸ› æ•…éšœæ’é™¤

### 1. ImportError: No module named 'gam'

ç¡®ä¿å·²å®‰è£… GAM æ¡†æ¶ï¼š

```bash
pip install -e .
```

### 2. OpenAI API é”™è¯¯

æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®è®¾ç½®ï¼š

```bash
echo $OPENAI_API_KEY
```

### 3. CUDA Out of Memory (ä½¿ç”¨ VLLM æ—¶)

å‡å°æ‰¹å¤„ç†å¤§å°æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š

```bash
--model meta-llama/Llama-3-8B  # ä½¿ç”¨æ›´å°çš„æ¨¡å‹
```

### 4. ä¸‹è½½ NLTK æ•°æ®å¤±è´¥

æ‰‹åŠ¨ä¸‹è½½ï¼š

```python
import nltk
nltk.download('punkt_tab')
```

## ğŸ“ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®æ–°çš„æ•°æ®é›†æ”¯æŒï¼

1. åœ¨ `eval/datasets/` åˆ›å»ºæ–°æ–‡ä»¶
2. ç»§æ‰¿ `BaseBenchmark` ç±»
3. å®ç°å¿…è¦çš„æ–¹æ³•
4. æ·»åŠ å¯¹åº”çš„ Shell è„šæœ¬
5. æ›´æ–°æœ¬ README

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªä¸ GAM æ¡†æ¶ç›¸åŒçš„è®¸å¯è¯ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹æ•°æ®é›†çš„ä½œè€…ï¼š
- HotpotQA
- NarrativeQA
- LoCoMo
- RULER

## ğŸ“® è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– Pull Requestã€‚

